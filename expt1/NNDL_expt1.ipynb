{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGdMjoBgPgdEwvE2F9VspC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephJonathanFernandes/Sem7-AI-ML-Honours-Lab-Codes/blob/main/NNDL_expt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aOZxGKKzuAr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e22a3fe-a02b-4869-b05a-a2a70a0c0452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [02:55<00:00, 974kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - train_loss: 1.5540, train_acc: 42.59%, val_loss: 1.1469, val_acc: 57.30%\n",
            "Epoch 2/20 - train_loss: 1.2494, train_acc: 55.01%, val_loss: 0.9792, val_acc: 64.56%\n",
            "Epoch 3/20 - train_loss: 1.1181, train_acc: 60.41%, val_loss: 0.8831, val_acc: 68.91%\n",
            "Epoch 4/20 - train_loss: 1.0443, train_acc: 63.21%, val_loss: 0.8765, val_acc: 69.58%\n",
            "Epoch 5/20 - train_loss: 0.9988, train_acc: 64.98%, val_loss: 0.8301, val_acc: 70.75%\n",
            "Epoch 6/20 - train_loss: 0.9565, train_acc: 66.57%, val_loss: 0.7883, val_acc: 72.16%\n",
            "Epoch 7/20 - train_loss: 0.9086, train_acc: 68.38%, val_loss: 0.7862, val_acc: 73.12%\n",
            "Epoch 8/20 - train_loss: 0.8786, train_acc: 69.63%, val_loss: 0.7147, val_acc: 74.50%\n",
            "Epoch 9/20 - train_loss: 0.8533, train_acc: 70.33%, val_loss: 0.7049, val_acc: 75.55%\n",
            "Epoch 10/20 - train_loss: 0.8318, train_acc: 71.21%, val_loss: 0.6636, val_acc: 76.77%\n",
            "Epoch 11/20 - train_loss: 0.7461, train_acc: 74.39%, val_loss: 0.6121, val_acc: 78.33%\n",
            "Epoch 12/20 - train_loss: 0.7209, train_acc: 75.03%, val_loss: 0.6040, val_acc: 78.42%\n",
            "Epoch 13/20 - train_loss: 0.7143, train_acc: 75.53%, val_loss: 0.5994, val_acc: 78.89%\n",
            "Epoch 14/20 - train_loss: 0.7038, train_acc: 75.74%, val_loss: 0.5964, val_acc: 79.17%\n",
            "Epoch 15/20 - train_loss: 0.6991, train_acc: 76.07%, val_loss: 0.5908, val_acc: 79.20%\n",
            "Epoch 16/20 - train_loss: 0.6909, train_acc: 76.36%, val_loss: 0.5888, val_acc: 79.26%\n",
            "Epoch 17/20 - train_loss: 0.6849, train_acc: 76.35%, val_loss: 0.5847, val_acc: 79.19%\n",
            "Epoch 18/20 - train_loss: 0.6784, train_acc: 76.62%, val_loss: 0.5815, val_acc: 79.46%\n",
            "Epoch 19/20 - train_loss: 0.6768, train_acc: 76.54%, val_loss: 0.5783, val_acc: 79.73%\n",
            "Epoch 20/20 - train_loss: 0.6722, train_acc: 76.85%, val_loss: 0.5736, val_acc: 79.80%\n",
            "Training completed in 47.17 minutes. Best val acc: 79.80%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 4 * 4, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_data_loaders(batch_size=128, data_dir='./data'):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "def train_epoch(model, device, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def eval_model(model, device, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def plot_curves(history, out_dir):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    epochs = len(history['train_loss'])\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, epochs + 1), history['train_loss'], label='train')\n",
        "    plt.plot(range(1, epochs + 1), history['val_loss'], label='val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss vs Epoch')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, epochs + 1), history['train_acc'], label='train')\n",
        "    plt.plot(range(1, epochs + 1), history['val_acc'], label='val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy vs Epoch')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_dir, 'training_curves.png'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    filename = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        torch.save(state, os.path.join(checkpoint_dir, 'best.pth'))\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Train CIFAR-10 with a simple CNN')\n",
        "    parser.add_argument('--epochs', default=20, type=int)\n",
        "    parser.add_argument('--batch-size', default=128, type=int)\n",
        "    parser.add_argument('--lr', default=1e-3, type=float)\n",
        "    parser.add_argument('--data-dir', default='./data', type=str)\n",
        "    parser.add_argument('--checkpoint-dir', default='./checkpoints', type=str)\n",
        "    parser.add_argument('--no-cuda', action='store_true')\n",
        "    parser.add_argument('--seed', default=42, type=int)\n",
        "    # Parse known arguments only, to ignore Colab's arguments\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    use_cuda = (not args.no_cuda) and torch.cuda.is_available()\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    if use_cuda:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    trainloader, testloader = get_data_loaders(batch_size=args.batch_size, data_dir=args.data_dir)\n",
        "\n",
        "    model = SimpleCNN(num_classes=10).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "    best_acc = 0.0\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_loss, train_acc = train_epoch(model, device, trainloader, criterion, optimizer)\n",
        "        val_loss, val_acc = eval_model(model, device, testloader, criterion)\n",
        "        scheduler.step()\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "        save_checkpoint({'epoch': epoch, 'state_dict': model.state_dict(), 'best_acc': best_acc, 'optimizer': optimizer.state_dict()}, is_best, args.checkpoint_dir)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{args.epochs} - train_loss: {train_loss:.4f}, train_acc: {train_acc:.2f}%, val_loss: {val_loss:.4f}, val_acc: {val_acc:.2f}%\")\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Training completed in {elapsed/60:.2f} minutes. Best val acc: {best_acc:.2f}%\")\n",
        "\n",
        "    plot_curves(history, out_dir=args.checkpoint_dir)\n",
        "\n",
        "    # Save final history\n",
        "    np.savez(os.path.join(args.checkpoint_dir, 'history.npz'), **history)\n",
        "\n",
        "\n",
        "main()"
      ]
    }
  ]
}